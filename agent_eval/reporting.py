"""
Report generation for TAT-QA agent evaluation.
Aggregates results from failure analysis and run logs to produce a comprehensive report,
including overall and per-type metrics, cost/latency analysis, and identified failure patterns.
"""
import json
import os

EXPERIMENTS_DIR = "./experiments"
RUN_LOG_PATH    = os.path.join(EXPERIMENTS_DIR, "run_log.json")
FAILURE_PATH    = os.path.join(EXPERIMENTS_DIR, "failure_report.json")
REPORT_PATH     = os.path.join(EXPERIMENTS_DIR, "report.json")

KNOWN_STRATEGIES = ["chain_of_thought", "zero_shot", "router"]


#   failure_report.json: { "{strategy}_{model}_predictions.json": { strategy, model, overall, by_type, total_failures, partial_matches, fail_by_type } }
def parse_key(key):
    """Extract strategy and model from filename, e.g. "chain_of_thought_gpt-4-120b_predictions.json"""
    base = key.replace("_predictions.json", "")
    for s in KNOWN_STRATEGIES:
        if base.startswith(s + "_"):
            return s, base[len(s) + 1:]
    return "unknown", base

# load failure report generated by failure_analysis.py
def load_failure_report():
    with open(FAILURE_PATH) as f:
        return json.load(f)

# load run logs generated by orchestrator.py, which contains cost/latency/tokens for each run
def load_run_log():
    """run_log.json 由 orchestrator 写入，包含 cost/latency/tokens"""
    if not os.path.exists(RUN_LOG_PATH):
        print("[warning] run_log.json not found — cost/latency will be missing")
        return {}
    with open(RUN_LOG_PATH) as f:
        data = json.load(f)
    # convert list of runs into dict keyed by "strategy__model" for easy lookup
    return {f"{r['strategy']}__{r['model'].replace('/', '-')}": r for r in data}

# build the final report by combining failure analysis results and run logs
def build_report():
    failure_data = load_failure_report()
    run_data     = load_run_log()

    runs = []
    for key, entry in failure_data.items():
        strategy = entry["strategy"]
        model    = entry["model"]
        lookup   = f"{strategy}__{model}"

        run_info = run_data.get(lookup, {})

        runs.append({
            "strategy":         strategy,
            "model":            model,
            "overall_em":       entry["overall"]["em"],
            "overall_f1":       entry["overall"]["f1"],
            "weighted_f1":      run_info.get("weighted_f1"),
            "total_tokens":     run_info.get("total_tokens"),
            "total_cost_usd":   run_info.get("total_cost_usd"),
            "cost_per_correct": run_info.get("cost_per_correct"),
            "avg_latency_s":    run_info.get("avg_latency_s"),
            "total_failures":   entry["total_failures"],
            "partial_matches":  entry["partial_matches"],
            "by_type":          entry["by_type"],
            "fail_by_type":     entry["fail_by_type"],
        })

    report = {
        "meta": {
            "dataset":    "TAT-QA dev",
            "n_per_type": 10,
            "total_n":    40,
            "models":     list({r["model"] for r in runs}),
            "strategies": list({r["strategy"] for r in runs}),
        },
        "runs": runs,
        "failure_patterns": [
            {
                "id":          "scale_contamination",
                "name":        "Scale/Unit Contamination",
                "description": "Model appends 'million' or '%' to numeric answers copied from table headers",
                "affected":    "arithmetic",
                "frequency":   "high",
                "fix":         "Post-process: strip units from numeric predictions",
            },
            {
                "id":          "span_truncation",
                "name":        "Span Truncation",
                "description": "Model paraphrases instead of exact-quoting source text",
                "affected":    "span",
                "frequency":   "medium",
                "fix":         "Prompt: instruct model to copy verbatim from source",
            },
            {
                "id":          "multispan_format",
                "name":        "Multi-span Format Mismatch",
                "description": "Correct values but wrong order, case, or delimiter → EM=0",
                "affected":    "multi-span",
                "frequency":   "high",
                "fix":         "Normalize: sort spans, lowercase, strip punctuation before eval",
            },
            {
                "id":          "count_unit",
                "name":        "Count Unit Leakage",
                "description": "Model adds unit to count answers ('2 years' vs '2')",
                "affected":    "count",
                "frequency":   "low",
                "fix":         "Post-process: extract first numeric token from count predictions",
            },
        ],
    }

    with open(REPORT_PATH, "w") as f:
        json.dump(report, f, indent=2)
    print(f"Report saved to {REPORT_PATH}")
    print(f"Runs included: {len(runs)}")
    for r in runs:
        cost_str = f"${r['total_cost_usd']:.4f}" if r['total_cost_usd'] else "n/a"
        lat_str  = f"{r['avg_latency_s']:.2f}s" if r['avg_latency_s'] else "n/a"
        print(f"  {r['strategy']:20s} | {r['model']:25s} | "
              f"F1={r['overall_f1']:.3f} | cost={cost_str} | latency={lat_str}")

    return report

if __name__ == "__main__":
    build_report()